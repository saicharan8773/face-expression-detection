{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzWROqXidSE2",
        "outputId": "5663939d-5ebb-4e2b-b3a2-72f3cbba4850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OouXY_a6dfks",
        "outputId": "fe6d8b29-de95-42ca-870e-8f687e5e9c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmjgJerUdhBc",
        "outputId": "bc54f012-c842-4a0d-81cf-84b66ddf5f3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml_sample\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ml_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBK5wxObdta1",
        "outputId": "f65459b1-c83a-47e0-b38d-989ab6236163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emotiondetector.h5    face-expression-recognition-dataset.zip  facialexpression.json\n",
            "emotiondetector.json  facialexpression.h5\t\t       kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAxzuUJddw-E",
        "outputId": "e6514010-cf53-4d64-dce4-07efabcc3e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: -r not specified; omitting directory '/content/drive/MyDrive/kaggle.json'\n",
            "401 - Unauthorized - Unauthenticated\n"
          ]
        }
      ],
      "source": [
        "!cp '/content/drive/MyDrive/kaggle.json' '/content/drive/MyDrive/kaggle.json'\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d jonathanoheix/face-expression-recognition-dataset\n",
        "# Unzip the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOZ0_Qdqdxf0",
        "outputId": "a0c67b95-5bcb-4082-bfc4-35e7c42a8742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emotiondetector.h5    face-expression-recognition-dataset.zip  facialexpression.json\n",
            "emotiondetector.json  facialexpression.h5\t\t       kaggle.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SVTfqMydy53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a240e35d-8261-401f-a260-7b22130a9176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/yt/images/images/train/angry/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip -q face-expression-recognition-dataset.zip -d /content/yt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lixLtuIYd02F"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Specify the file path\n",
        "image_path = \"/content/yt/images/images/train/angry/1.jpg\"\n",
        "\n",
        "# Display the image\n",
        "display(Image(filename=image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcdIhPywd7WJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdbvItlJe6_f"
      },
      "outputs": [],
      "source": [
        "TRAIN_DIR = \"/content/yt/images/train\"\n",
        "TEST_DIR = \"/content/yt/images/validation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03f0RpWOe8mn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def create_dataframe(dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for label in os.listdir(dir):\n",
        "        for imagename in glob.glob(os.path.join(dir, label, '*.*')):\n",
        "            image_paths.append(imagename)\n",
        "            labels.append(label)\n",
        "        print(label, \"completed\")\n",
        "\n",
        "    data = {'Image_Path': image_paths, 'Label': labels}\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlpKQgdbe9sK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "image_paths, labels = create_dataframe(TRAIN_DIR)\n",
        "\n",
        "train = pd.DataFrame()\n",
        "train['image'] = image_paths\n",
        "train['label'] = labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACKbrC2Ue-mt"
      },
      "outputs": [],
      "source": [
        "print(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxCkat6jfACe"
      },
      "outputs": [],
      "source": [
        "image_paths, labels = create_dataframe(TEST_DIR)\n",
        "\n",
        "test = pd.DataFrame()\n",
        "test['image'] = image_paths\n",
        "test['label'] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FLAPzT-fDeS"
      },
      "outputs": [],
      "source": [
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCUBQRw_fEoj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "def extract_features(images):\n",
        "    features = []\n",
        "\n",
        "    for image_path in tqdm(images):\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        img = cv2.resize(img, (48, 48))  # Resize the image to the desired shape\n",
        "        img = img.reshape((48, 48, 1))  # Add channel dimension\n",
        "        features.append(img)\n",
        "\n",
        "    features = np.array(features)\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0ddiE5xfGHH"
      },
      "outputs": [],
      "source": [
        "train_features = extract_features(train['image'])\n",
        "print(train_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlJycH8BfHh_"
      },
      "outputs": [],
      "source": [
        "test_features = extract_features(test['image'])\n",
        "print(test_features.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-8ddib-fJEU"
      },
      "outputs": [],
      "source": [
        "x_train = train_features/255.0\n",
        "x_test = test_features/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SGKOQSjhfKYb",
        "outputId": "b6d262ef-ce59-480b-81d1-3cea4c4f8eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.1.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              },
              "id": "f5653997ab264e8b9d329e4f02500f3f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YxOdPPfUfLnB"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "XiLe_CZdfMzw",
        "outputId": "39eb120c-d30b-4100-8016-87704d05cc58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(train['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NjtQ6M4-fN8o"
      },
      "outputs": [],
      "source": [
        "y_train = le.transform(train['label'])\n",
        "y_test = le.transform(test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fgcUiGlefPDN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "9c817d81-cbe3-458f-cd5d-2d721a2e1695"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "zero-size array to reduction operation maximum which has no identity",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4af32c798bf6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m     \"\"\"\n\u001b[0;32m-> 2810\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2811\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
          ]
        }
      ],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPRZO82sfP75"
      },
      "outputs": [],
      "source": [
        "input_shape = (48, 48, 1)\n",
        "num_classes = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q6j5BUNfQz3"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional Layers\n",
        "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected Layers\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(num_classes, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9edXfhnfRsN"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqrGeCVJfSvm"
      },
      "outputs": [],
      "source": [
        "model.fit(x=x_train, y=y_train, batch_size=120, epochs=10, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuVczbwskDn4"
      },
      "outputs": [],
      "source": [
        "model.fit(x=x_train, y=y_train, batch_size=120, epochs=10, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5hBPzmTkrC3"
      },
      "outputs": [],
      "source": [
        "model.fit(x=x_train, y=y_train, batch_size=120, epochs=10, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrAgygZjfT1t"
      },
      "outputs": [],
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"emotiondetector.json\",'w') as json_file:\n",
        "    json_file.write(model_json)\n",
        "model.save(\"emotiondetector.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEzRgt_whJgI"
      },
      "outputs": [],
      "source": [
        "from keras.models import model_from_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W97PhgqshYYh"
      },
      "outputs": [],
      "source": [
        "json_file = open(\"/content/drive/MyDrive/ml_sample/facialexpression.json\", \"r\")\n",
        "model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(model_json)\n",
        "model.load_weights(\"/content/drive/MyDrive/ml_sample/facialexpression.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hhOI4I9gyGp"
      },
      "outputs": [],
      "source": [
        "label = ['angry','disgust','fear','happy','neutral','sad','surprise']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEfOaAKuzLkf"
      },
      "outputs": [],
      "source": [
        "def ef(image):\n",
        "    img = load_img(image,grayscale =  True )\n",
        "    feature = np.array(img)\n",
        "    feature = feature.reshape(1,48,48,1)\n",
        "    return feature/255.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0gN9JRSzMK0"
      },
      "outputs": [],
      "source": [
        "image = '/content/yt/images/train/fear/10010.jpg'\n",
        "print(\"original image is of fear\")\n",
        "img = ef(image)\n",
        "pred = model.predict(img)\n",
        "pred_label = label[pred.argmax()]\n",
        "print(\"model prediction is \",pred_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMsZNhpWzl1H"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaG_LR7x_2To"
      },
      "outputs": [],
      "source": [
        "image = '/content/yt/images/train/fear/10010.jpg'\n",
        "print(\"original image is of fear\")\n",
        "img = ef(image)\n",
        "pred = model.predict(img)\n",
        "pred_label = label[pred.argmax()]\n",
        "print(\"model prediction is \",pred_label)\n",
        "plt.imshow(img.reshape(48,48),cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOgAO8Wy_-Mk"
      },
      "outputs": [],
      "source": [
        "image = '/content/yt/images/train/angry/0.jpg'\n",
        "print(\"original image is of fear\")\n",
        "img = ef(image)\n",
        "pred = model.predict(img)\n",
        "pred_label = label[pred.argmax()]\n",
        "print(\"model prediction is \",pred_label)\n",
        "plt.imshow(img.reshape(48,48),cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N88Fwy1raH0_"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrY510YDawgB"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless\n",
        "!pip install requests\n",
        "import cv2\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "image_link = \"https://t4.ftcdn.net/jpg/00/56/93/53/360_F_56935312_NiqxkRKOdGSJd86Tc2uLycL9fkUsIlRW.jpg\"\n",
        "response = requests.get(image_link)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0CTi-_qNHnl"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import requests\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load the model architecture from JSON file\n",
        "json_file = open(\"/content/drive/MyDrive/ml_sample/facialexpression.json\", \"r\")\n",
        "model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "# Load the model weights\n",
        "model = model_from_json(model_json)\n",
        "model.load_weights(\"/content/drive/MyDrive/ml_sample/facialexpression.h5\")\n",
        "\n",
        "# Haar cascade file for face detection\n",
        "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "face_cascade = cv2.CascadeClassifier(haar_file)\n",
        "\n",
        "def extract_features(image):\n",
        "    feature = np.array(image)\n",
        "    feature = feature.reshape(1, 48, 48, 1)\n",
        "    return feature / 255.0\n",
        "\n",
        "# Provide the image link\n",
        "image_link = \"https://img.freepik.com/free-photo/young-bearded-man-with-white-t-shirt_273609-7200.jpg?size=626&ext=jpg&ga=GA1.1.1395880969.1709856000&semt=ais\"\n",
        "\n",
        "# Download the image from the link\n",
        "response = requests.get(image_link)\n",
        "image = Image.open(BytesIO(response.content))\n",
        "\n",
        "# Convert PIL image to OpenCV format\n",
        "image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces in the image\n",
        "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
        "\n",
        "# Process each face detected in the image\n",
        "for (p, q, r, s) in faces:\n",
        "    face = gray[q:q+s, p:p+r]\n",
        "    cv2.rectangle(image, (p, q), (p+r, q+s), (255, 0, 0), 2)\n",
        "    face = cv2.resize(face, (48, 48))\n",
        "    img = extract_features(face)\n",
        "    pred = model.predict(img)\n",
        "    prediction_label = labels[pred.argmax()]\n",
        "    cv2.putText(image, '%s' % (prediction_label), (p-10, q-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 0, 255))\n",
        "\n",
        "# Display the processed image\n",
        "cv2_imshow(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo0drJmdl6m8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Open the default webcam (index 0)\n",
        "webcam = cv2.VideoCapture(0)\n",
        "\n",
        "# Check if the webcam is opened successfully\n",
        "if not webcam.isOpened():\n",
        "    print(\"Error: Failed to open webcam.\")\n",
        "    exit()\n",
        "\n",
        "# Main loop for video capture and display\n",
        "while True:\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = webcam.read()\n",
        "\n",
        "    # Check if the frame is retrieved successfully\n",
        "    if not ret:\n",
        "        print(\"Error: Failed to retrieve frame from webcam.\")\n",
        "        break\n",
        "\n",
        "    # Display the captured frame\n",
        "    cv2.imshow(\"Webcam\", frame)\n",
        "\n",
        "    # Wait for 'Esc' key press to exit\n",
        "    key = cv2.waitKey(1)\n",
        "    if key == 27:  # 'Esc' key\n",
        "        break\n",
        "\n",
        "# Release the webcam and close OpenCV windows\n",
        "webcam.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLakc8n0kspk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "\n",
        "# Load model from JSON and weights from H5 file\n",
        "json_file = open(\"/content/drive/MyDrive/ml_sample/facialexpression.json\", \"r\")\n",
        "model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(model_json)\n",
        "model.load_weights(\"/content/drive/MyDrive/ml_sample/facialexpression.h5\")\n",
        "\n",
        "# Load Haar cascade for face detection\n",
        "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "face_cascade = cv2.CascadeClassifier(haar_file)\n",
        "\n",
        "# Function to extract features from image\n",
        "def extract_features(image):\n",
        "    feature = np.array(image)\n",
        "    feature = feature.reshape(1, 48, 48, 1)\n",
        "    return feature / 255.0\n",
        "\n",
        "# Webcam setup\n",
        "webcam = cv2.VideoCapture(0)\n",
        "webcam = cv2.VideoCapture(0)\n",
        "webcam = cv2.VideoCapture(0)\n",
        "webcam = cv2.VideoCapture(0)\n",
        "\n",
        "# Emotion labels\n",
        "labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
        "\n",
        "# Main loop for video capture and emotion recognition\n",
        "while True:\n",
        "    ret, frame = webcam.read()\n",
        "    ret, frame = webcam.read()\n",
        "    ret, frame = webcam.read()\n",
        "\n",
        "    if not ret:  # Check if frame retrieval was successful\n",
        "        print(\"Error: Failed to retrieve frame from webcam.\")\n",
        "        break\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Perform face detection\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    try:\n",
        "        # Process each detected face\n",
        "        for (x, y, w, h) in faces:\n",
        "            face_img = gray[y:y+h, x:x+w]  # Extract face region\n",
        "            face_img = cv2.resize(face_img, (48, 48))  # Resize to model input size\n",
        "            features = extract_features(face_img)  # Extract features\n",
        "            pred = model.predict(features)  # Perform prediction\n",
        "            prediction_label = labels[pred.argmax()]  # Get predicted label\n",
        "            cv2.putText(frame, prediction_label, (x-10, y-10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 0, 255))\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)  # Draw rectangle around face\n",
        "\n",
        "        # Display the processed frame\n",
        "        cv2.imshow(\"Emotion Recognition\", frame)\n",
        "\n",
        "        # Check for 'Esc' key press to exit the loop\n",
        "        key = cv2.waitKey(1)\n",
        "        if key == 27:  # 'Esc' key\n",
        "            break\n",
        "    except cv2.error:\n",
        "        pass\n",
        "\n",
        "# Release the webcam and close all OpenCV windows\n",
        "webcam.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scJZiqLoZmCK"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless\n",
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "cap = cv2.VideoCapture(1)\n",
        "print(cap.isOpened())\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "5ixPzxl2Fd3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Open the default camera (index 0)\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Check if the camera is opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open camera.\")\n",
        "else:\n",
        "    try:\n",
        "        # Capture a single frame from the camera\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # Check if the frame is captured successfully\n",
        "        if ret:\n",
        "            # Display the captured frame in the notebook\n",
        "            display(Image(data=cv2.imencode('.jpg', cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))[1].tobytes(), format='png'))\n",
        "\n",
        "            # Save the captured frame to a file\n",
        "            cv2.imwrite('captured_frame.jpg', frame)\n",
        "            print(\"Captured frame saved as 'captured_frame.jpg'.\")\n",
        "\n",
        "            # Read and display the saved frame\n",
        "            saved_frame = cv2.imread('captured_frame.jpg')\n",
        "            display(Image(data=cv2.imencode('.jpg', cv2.cvtColor(saved_frame, cv2.COLOR_BGR2RGB))[1].tobytes(), format='png'))\n",
        "\n",
        "        else:\n",
        "            print(\"Error: Could not capture frame.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Release the camera\n",
        "        cap.release()\n"
      ],
      "metadata": {
        "id": "NvE3WzsCEoZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wLLMTm3aEuvl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}